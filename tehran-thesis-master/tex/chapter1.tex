% !TeX root=../main.tex

\chapter{مقدمه}
% دستور زیر باعث عدم‌نمایش شماره صفحه در اولین صفحهٔ این فصل می‌شود.
%\thispagestyle{empty}
\section{معرفی ویژگی‌های متمایزکننده‌ی شبکه‌ی اینترنت اشیا}
رشد شبکه های اینترنت اشیا کاربردهای زیادی از جمله سیستم پایش سلامت \footnote{\lr{health-care system}}،‌ خانه‌ی هوشمند \footnote{smart home} و شهر هوشمند \footnote{smart city}را توسعه بخشیده است. بخش زیربنایی کاربردهای مبتنی بر شبکه‌های اینترنت اشیا، اندازه‌گیری و گزارش دیتاهایی مانند دما، رطوبت، ویدیو و ... می‌باشد. این اطلاعات از طریق درگاه‌\footnote{gateway}ها به گره‌های مصرف کننده می‌رسد. افزایش تعداد گره‌های مصرف کننده و نیز تعداد حسگرها ترافیک عظیمی در شبکه‌های اینترنت اشیا را ایجا کرده و نیز میزان تراکم در این شبکه‌ها را بالا می‌برد. این مسئله باعث کاهش میزان کیفیت سرویس می‌شود. به علاوه اینکه فعالسازی دوباره‌ی حسگرها برای واکشی پاسخ، میزان انرژی مصرفی آنها را بالا برده و از عمر باتری آنها کم می‌کند.

شبکه‌های اینترنت اشیا دارای دو ویژگی متمایز و چالش‌زا می‌باشند. اول اینکه تأمین انرژی اکثر تجهیزات و حسگرها از طریق باتری تأمین می‌شود و بنابراین با انرژی محدود کار می‌کنند و بالا بودن حچم درخواست‌ها باعث می‌شود تا از دست دادن انرژی در آنها سریعتر رخ دهد. دوم اینکه دیتای گزارش شده توسط حسگرها برای مدت زمان کوتاهی معتبر یا به عبارتی تازه می‌باشد و پس از مدتی منقضی می‌شود. در بعضی از مقالات به ویژگی دوم تحت عنوان تازگی دیتا\footnote{\lr{data freshness}} و در بعضی دیگر تحت یک مفهوم کلی تر با نام سن اطلاعات\footnote{\lr{Age of Information (AoI)}} اشاره شده است. 
\subsection{استراتژی ذخیره‌سازی}
حال ذخیره سازی به عنوان راه حلی برای متوازن‌سازی ترافیک در شبکه مطرح می‌شود به این معنا که در درگاه‌های شبکه‌های اینترنت اشیا یک حافظه‌ی ذخیره‌سازی قرار گرفته است که اطلاعات را جمع‌آوری می‌کند و می‌تواند به طور موقت دیتا را ذخیره کرده و در صورت ارسال درخواست از سوی گره‌های مصرف کننده، دیتای مربوطه را برای آنها بفرستد. 

در سناریوی توصیف شده از ارتباط مستقیم لینک‌های پشتی\footnote{\lr{backhaul links}} با حسگرها جلوگیری کرده‌ایم که در اثر آن ترافیک شبکه معادلتر شده و حجم درخواست‌های ارسالی به یک حسگر کاهش می‌یابد و بنابراین حسگر به تعداد دفعات کمتری برای واکشی پاسخ فعال می‌شود و مصرف انرژی در آن کاهش‌ می‌یابد. در سناریوی مطرح شده باید قیدی را مبنی بر محدودیت حافظه‌ی ذخیره‌سازی در نظر داشته باشیم به علاوه‌ی اینکه به طراحی یک سیاست مناسب برای ذخیره‌سازی محتوا با در نظر گرفتن فاکتور تازگی دیتا نیاز داریم.  

تعداد قابل توجهی از تحقیقات تا قبل از سال 2018 به یک استراتژی ذخیره‌سازی مبتنی بر محتوا\footnote{\lr{ICN caching}} برای یک نیاز خاص در شبکه‌های اینترنت اشیا پرداخته‌اند و روش‌هایی را مبتنی بر طبیعت \lr{ad-hoc} و بی‌سیم شبکه‌های اینترنت اشیا ارائه کرده‌اند. 

عموماً یک استراتژی ذخیره‌سازی می‌تواند بر دو جنبه اثر بگذارد: یکی اینکه کدام محتوا در حافظه‌ی کش ذخیره گردد که به عبارتی به تصمیم برای ذخیره‌ی محتوا\footnote{\lr{caching decision}} می‌پردازد و دیگری آنکه در صورتیکه ظرفیت حافظه‌ی کش پر شود، الویت با حذف کدام یک از محتواها می‌باشد و یا به عبارتی مکانیزم یا سیاست جایگزینی محتوا\footnote{\lr{cache replacement policy}} را تحت تأثیر قرار می‌دهد. 

\subsubsection{\lr{LCE(Leave Copy Everywhere)}}
سر راست ترین استراتژی ذخیره‌سازی \lr{CEE} یا به تعبیری \lr{LCE (Leave Copy Everywhere)} می‌باشد. در این حالت تمامی گره‌های مسیر برای ذخیره‌سازی محتوای ورودی تلاش می‌کنند. در شبکه‌های مرسوم مبتنی بر محتوا، با توجه به این موضوع که به طور پیش‌فرض محتوا حجم زیادی دارد، این روش با اغماض قابل قبول بوده و سربار کمی نیز ایجاد می‌کند و برتری اصلی آن در اینست که سریعترین انتشار ممکن از محتوا را پیشنهاد می‌دهد.

در این استراتژی گره‌های مسیر هر محتوایی را که شنود می‌کنند، بلافاصله در حافظه ذخیره می‌نمایند. این مسئله وجود کپی از محتوا را تضمین می‌نماید ولی باعث می‌شود تا بیش از حد مورد نیاز کپی از آن محتوا وجود داشته باشد در حالیکه از قابلیت‌ها و منابع شبکه به درستی استفاده نکرده‌ایم. عملاً یکی از اهداف در این مسئله کاهش موارد زائد و بالا بردن تنوع می‌باشد یعنی انتظار داریم تا در حافظه‌های مختلف، چینش‌های متفاوتی از محتواهای موجود ذخیره شده باشند. 
\subsubsection{\lr{ProbCache caching}}
دو پیاده‌سازی استاتیک و دینامیک برای این استراتژی ذخیره‌سازی وجود دارد. در استراتژی استاتیک، یک مقدار ثابت \lr{p} و بتبع بین 0 و 1 برای احتمال ذخیره سازی در گره‌های مسیر درنظر گرفته می‌شود. با گذر محتوای جدید از هر گره در مسیر، آن گره یک عدد رندوم تولید کرده و در صورتیکه این مقدار از \lr{p} کمتر باشد، محتوا را ذخیره می‌کند و در غیر اینصورت آن را تنها جلو می‌راند. 

در این شرایط محتوای محبوب تر که در طول زمان با نرخ بیشتری درخواست می‌شود،‌ شانس بالاتری برای ذخیره سازی پیدا می‌کند. در واقع احتمال ذخیره‌سازی محتوا بعد از \lr{n} بار دریافت شدن به صورت $1-(1-p)^n$ محاسبه می‌شود که با افزایش \lr{n} این مقدار به 1 میل می‌کند. یعنی هر چه میزان محبوبیت فایل یا محتوا بیشتر باشد، احتمال ذخیره‌ی آن نیز افزایش می‌یابد. 

در حالت دینامیک احتمال ذخیره کردن محتوا در یک گره براساس پارامترهایی مانند تازگی،‌ نوع یا تولیدکننده‌ی محتوا، میزان سطح انرژی گره، توپولوژی شبکه و گره های مجاور آن و یا سایر اطلاعات محلی ایجاد می‌گردد.

\subsubsection{\lr{Betweenness Centrality Caching}}
در این استراتژی پارامتری براساس توپولوژی شبکه برای هر گره محاسبه می‌شود. درواقع تعداد بارهایی که آن گره در مسیرهایی بین فرستده و گیرنده قرار می‌گیرد، محاسبه شده و بسته به توپولوژی در جایی که این پارامتر بیشینه می گردد،‌ یک کپی از محتوا ذخیره می‌شود.

\subsubsection{\lr{LCD(Leave Copy Down)}}
در این استراتژی ذخیره‌سازی یک کپی و آن هم در نزدیکترین گره به سنسور، یعنی در درگاه متصل به سنسور قرار داده می‌شود. در این حالت کپی‌های زائد از محتوای مدنظر در سایر گره‌ها قرار ندارد، درواقع نسبت به حالت‌های قبلی از این حیٍث استفاده‌ی بهینه‌تری از منابع کرده‌ایم اما همچنان برای واکش پاسخ، درخواست ارسال شده تا گره یکی مانده به سنسور بالا می‌رود که این مسئله چندان مطلوب نیست.

\subsubsection{\lr{Edge Caching}}
در استراتژی ذخیره‌سازی لبه مانند حالت \lr{LCD} یک کپی از محتوا ولی دقیقاً در خلاف جهت آن یعنی در نزدیکترین گره به مصرف کننده یا در دستگاه کاربر ذخیره می‌گردد که عملا مشکل حالت قبلی در اینجا رفع شده است یعنی درخواست برای واکشی پاسخ تا حسگر یا گره‌های نزدیک به آن بالا نمی‌رود و عملاً از جحم ترافیک شبکه کم می‌شود.

\paragraph{}
در شکل \ref{fig:cachingStrategy} استراتژی‌های متداول در شبکه‌های مبتنی بر محتوا را (که پیشتر به آنها اشاره شد،) مشاهده می‌کنید.
\begin{figure}[ht]
	\centerline{\includegraphics[width=13cm]{cachingStrategy}}
	\caption{استراتژی های متداول ذخیره‌سازی در شبکه‌های مبتنی بر محتوا برگرفته از\\ \lr{\href{https://mdpi-res.com/d_attachment/futureinternet/}{future internet}}}
	\label{fig:cachingStrategy}
\end{figure}

\subsection{سیاست جایگزینی محتوا}
در عمل به دلیل سایز محدود حافظه‌ی ذخیره‌سازی به یک سیاست مناسب برای جایگزینی محتوا در حافظه نیاز داریم. درواقع اصلی ترین هدف ما، یافتن بهینه ترین سیاست در تعامل با محیط می‌باشد به گونه‌ای که میزان تأخیر در ارسال دیتا برای یک محتوا را کمینه نماید.

  قطعاً استراتژی ذخیره‌سازی برای بعضی از محتویات که در طول زمان هیچ درخواستی برای آنها در شبکه وجود ندارد‌ و قبل از انتظار ما برای برآوردن درخواست دیتای مربوط به آنها منقضی می‌شود، مطلوب ما نیست. به عبارتی می توان گفت که در این حالت یک سیاست جایگزینی محتوای مناسب به کارایی استراتژی ذخیره‌‌سازی انتخاب شده، ‌کمک خواهد کرد. یک سری روش رایج برای شبکه‌های \lr{NDN} تعریف گردیده است که برجسته‌ترین و قدیمی‌ترین سیاست مطرح شده، \lr{FIFO (First Input, First Output)} می‌باشد.
  
  سیاست‌های جایگزینی مرسوم،‌ به چهار دسته مطابق با جدول\eqref{tab:replacementPolicies} تقسیم می‌گردند که در ادامه به آنها می‌پردازیم.
  \begin{table}[ht]
  	\caption{سیاست‌های مرسوم برای جایگزینی محتوا}
  	\label{tab:replacementPolicies}
  	\centering
  	\onehalfspacing
  	\begin{tabular}{|r|c|l|r|}
  		\hline \lr{Existing Plicies} & \lr{Description} & \lr{Category} \\ 
  		\hline \lr{LRU, LRU-threshold,} & \lr{Keeps the recently referenced objects} & \lr{Recency-based} \\
  			   \lr{LRU-hot, SLRU,} & \lr{} & \lr{} \\ 
  			   \lr{HLRU, LRU-LSC} & \lr{} & \lr{} \\ 
  		\hline \lr{LFU, LFU-Aging,} & \lr{Keeps the most requested objects} & \lr{Frequency-based} \\ 
  			\lr{LFU-DA, swLFU} & \lr{} & \lr{} \\
  		\hline \lr{RR, RAND,} & \lr{A simple random choice} & \lr{Randomized Policies} \\ 
  		\lr{HARMONIC} & \lr{to avoid computation overhead} & \lr{} \\
  		\hline \lr{SIZE, PSS, CSS} & \lr{Evicts large contents} & \lr{Size-based} \\ 
  		\hline 
  	\end{tabular} 
  \end{table}
  
  \subsubsection{\lr{Recency-Based Replacement Policies}}
  معروف‌‌ترین و پرکاربردترین سیاست از میان سیاست‌های جایگزینی محتوا در این دسته، سیاست \lr{LRU(Least Recently Used)} می‌باشد. (می توان گفت که سایر سیاست‌های متعلق به این دسته از سیاست \lr{LRU} مشتق شده‌اند.) در این حالت هریک از گره‌ها مجموعه‌ی درخواست‌های ارسال شده برای هریک از محتوا را دنبال می‌کند و زمانیکه نیاز به جایگزینی محتوا برای ذخیره‌ی محتوای جدید باشد، محتوایی را که اخبراً از آن کمتر از سایرین استفاده شده است را از حافظه دور می‌ریزد و فایل جدید را ذخیره می‌کند. روش مطرح شده نسبت به سایر سیاست‌های جایگزینی، کارایی بسیار بالایی دارد، اما در شرایطی که تعداد محتویات محبوب از سایز حافظه بیشتر باشد، شبکه دچار اختلال\footnote{\lr{thrashing}} خواهد شد.
  
  \subsubsection{\lr{Frequency-Based Replacement Policies}}
  سیاست اصلی در این گروه، سیاست \lr{LFU(Least Frequency Used)} می‌باشد. در واقع برخلاف سیاست \lr{LRU} که به سوال درباره‌ی آخرین بار استفاده از محتوا پاسخ می‌دهد،  سیاست \lr{LFU} میزان درخواست برای دیتای محتوا را دنبال می‌کند و در نهایت محتوا با کمترین میزان فرکانس درخواست را از حافظه حذف کرده و محتوای جدید را جایگزین آن می‌نماید و نسبت به سایر سیاست‌های جایگزینی، تنوع\footnote{\lr{diversity}} خوبی را برقرار می‌سازد اما بار بیشتری را در شبکه ایجاد می‌کند زیرا به جای آنکه فایل‌های محبوب‌تر را ذخیره نماییم، مجبور به ذخیره‌ی دیتای تازه هستیم یعنی به صورت مداوم برای واکشی پاسخ به حسگر مربوطه درخواست ارسال می‌نماییم که این امر باعث تخلیه‌ی باتری آن خواهد شد.
  
  \subsubsection{\lr{Randomized Replacement Policies}}
  این دسته از سیاست‌ها، سیاست‌های رندوم می‌باشند که در صورت تکمیل ظرفیت کش، از میان محتویات ذخیره شده یکی را به صورت رندوم انتخاب کرده و حذف می‌نماید و سپس محتوای جدید را جایگزین خواهد نمود. از این سیاست به عنوان معیاری\footnote{\lr{benchmark}} برای سنجش سایر سیاست‌ها استفاده می‌شود.
  
  \subsubsection{\lr{Size-Based Replacement Policies}}
  دامنه‌ی اصلی کاربرد این دسته از سیاست‌ها، ذخیره‌سازی محتوای وب می‌باشد که براساس سایز محتوا ذخیره‌سازی را انجام می‌دهد و سعی می‌شود که تا حدممکن محتوای با سایز کمتر را ذخیره نماید.
 
\subsection{متریک‌های مسئله}
برای مقایسه‌ی استراتژی‌های مختلف متریک‌های متعددی وجود دارد که غالباً شناخته شده نیز می‌باشند و شاید بتوان گفت که متریک‌هایی که در ادامه مطرح می‌شوند، کاملاً از یکدیگر مستقل نیستند. اولین متریک مطرح در یک مسئله‌ی ذخیره‌سازی بار سرور\footnote{\lr{server load}} و نسبت موفقیت حافظه‌ی کش در واکشی پاسخ\footnote{\lr{cache hit ratio}} می‌باشد. در شبکه‌های مبتنی بر محتوا منظور از موفقیت حافظه در واکشی پاسخ، زمانیست که مقدار دیتای مدنظرمان از روی مقادیر ذخیره شده در حافظه، واکشی شود. حال ضریب واکشی پاسخ از سرور زمانی رخ می‌دهد که دیتای مدنظرمان در حافظه‌ی کش ذخیره نشده باشد و مجبور باشیم تا مقدار آن را از گره تولیدکننده واکشی نماییم و بنابراین درخواست تا حسگر بالا رفته و اصطلاحاً بار سرور نیز افزایش می‌یابد. در واقع می توان گفت که دو پارامتر مطرح شده دوگان یکدیگر می‌باشند بدین معنا که با بالا رفتن میزان موفقیت حافظه‌ی کش در واکشی پاسخ، سرور مقدار بار کمتری را متحمل می‌شود.

\paragraph{}
متریک مطرح دیگر تأخیر بازیابی دیتا\footnote{\lr{data retrieval delay}} می‌باشد که به میانگین زمانی لازم برای واکشی دیتای مدنظرمان اطلاق می‌شود که باز ارسال‌های احتمالی را نیز شامل می‌گردد و بتبع این متریک می‌تواند تابعی از ازدحام\footnote{\lr{congestion}} در شبکه و یا تنوع در محتویات ذخیره شده باشد. هرچقدر که میزان تنوع در محتویات ذخیره شده بیشتر باشد، عملاً زمان کمتری برای واکشی پایخ نیاز است. البته بالا بودن تنوع بیش از حدمجاز باعث می‌شود تا بی دلیل یک سری از محتویاتی را که به آنها نیازی نداریم، در حافظه ذخیره نماییم. درواقع برای اینکه بگوییم یک استراتژی ذخیره‌سازی تا چه میزان کارآمد می‌باشد، باید ببینیم که با چه سرعتی محتوای مدنظر را برایمان واکشی می نماید.  

\paragraph{}
متریک سوم نسبت بازارسال فایل‌های خواسته شده به دلیل اتمام مهلت\footnote{\lr{timed out}} آن به تمامی ارسال‌های موفق و ناموفق\footnote{\lr{interest retransmission ratio}} می‌باشد که عملاً مفهوم مستقلی از تأخیر و پارامتر قبلی نیست. درواقع همانطور که پیشتر نیز بدان اشاره شده، این متریک نیز تابعی از ازدحام و تنوع در محتویات ذخیره شده می‌باشد و علاوه براین به کارایی استراتژی ذخیره شده از سوی ما نیز بستگی دارد. در یک استراتژی ذخیره‌سازی ایده‌آل، تنوع تا حدمجاز بالا رفته و در ارسال های اولیه دیتای مدنظر از حافظه یا حسگر (بنا بر کارایی استراتژی به کار گرفته) واکشی می شود. درواقع این امکان وجود دارد که یک کپی از فایل مدنظرمان در گره‌های میانی ذخیره شده باشد و بتبع هرچه این گره به مصرف کننده نزدیکتر باشد، زمان کمتری برای بازارسال نیاز بوده و بتبع ترافیک کمتری نیز درشبکه خواهیم داشت. 

\paragraph{}
متریک چهارم میزان کل دیتای حذف شده از حافظه\footnote{\lr{total cache eviction}} می‌باشد که بیانگر سازگاری استراتژی انتخاب شده با میزان محبوبیت و پراکندگی توزیع آن در شبکه می‌باشد. حال هرچه‌قدر که استراتژی ذخیره‌سازی انتخاب شده موجب اختلال بیشتری در سیستم شود، میزان راندن محتویات از حافظه بالاتر می‌رود که نشان می‌دهد فایل‌های اولیه‌ی ذخیره شده در کش از محبوبیت کافی برخوردار نبوده یا حتی ممکنست این اتفاق به دلیل کم بودن حافظه‌ی اختصاص داده برای ذخیره‌سازی محتویات محبوب باشد. درواقع حذف یکسری از محتویات از حافظه گریزناپذیر می‌باشد ولی با اختصاص یک استراتژی مناسب می توان مقدار آن را متعادلتر نمود.

\paragraph{}
متریک تنوع در ذخیره‌سازی\footnote{\lr{cache diversity}}، بیانگر گوناگونی بین محتویات ذخیره شده می‌باشد و به صورت 
\begin{equation}\label{eq1}
	D = \frac{|C_{disj}|}{|S|}
\end{equation}
تعریف می‌گردد. مقدار \lr{S} تعداد کل محتویات تولید شده توسط گره تولیدکننده می‌باشد و هر گره تولیدکننده حداقل یک محتوا در جایی از شبکه دارد و مقدار 
\lr{$C_{disj}$} 
تعداد پیشوندهای متمایز برای تمامی مقادیر کش شده را مشخص می‌کند. این متریک تنوع در ذخیره‌سازی محتوا برای یک گره تولیدکننده را تا حد قابل قبولی تخمین می‌زند ولی در اینجا به دنبال متریکی برای بیا تنوع در سطح محتوا هستیم تا تعداد محتویات یکتا  و متمایز ذخیره شده را مشخص نماییم.

\paragraph{}
با توجه به توضیحات داده شده، برای مشخص کردن تنوع در سطح محتوا، متریک  نسبت نگهداری در حافظه\footnote{\lr{cache retention ratio}} معرفی می‌شود که متریک تنوع را تکمیل می‌نماید. درواقع این متریک نسبت محتویات متمایز ذخیره شده در کش را در یک لحظه‌ی خاص به کل محتویات متمایز در طول عمر شبکه نشان می‌دهد و با رابطه‌ی 
\begin{equation}\label{eq1}
	D = \frac{D_q}{D_p}
\end{equation}
به دست می‌آید. 

\section{استفاده از یادگیری تقویتی عمیق در تعیین سیاست جایگزینی محتوا}

